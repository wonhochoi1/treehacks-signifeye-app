# Signifeye
Signifeye aims to create convenience for patients with visual disabilities, one sense at a time. Code entails hardware coding for the Arduino boards along with an accomodating React Native application that can be used by both the patients and patient assistants with haptic feedback manipulation. Developed as part of the TreeHacks 2023.

![Signifeye Flowchart](https://user-images.githubusercontent.com/113927868/231316621-ce64d1d7-9b5e-4fbc-985d-dcd66d470015.png)
Image 1. The app itself entails a remote control and dashboard for patient tracking, with the remote control side of the app focusing more on haptic feedback for the patient. 
![Signifeye Gallery](https://user-images.githubusercontent.com/113927868/231316737-0e2d9582-6f6d-4ebd-8a0f-253927a0fa43.jpeg)

Image 2. The mockup of the actual hardware that uses ultrasonic sensors on the glasses that the patient would be wearing. The sensors are coded to different auditory tones that the patient would hear.
